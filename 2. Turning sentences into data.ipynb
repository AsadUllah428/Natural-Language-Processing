{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b463af55",
   "metadata": {},
   "source": [
    "## Description: Text Preprocessing with Keras Tokenizer and Padding\n",
    "This script demonstrates essential text preprocessing steps for Natural Language Processing using TensorFlow's Keras Tokenizer and pad_sequences.\n",
    "\n",
    "It begins by defining a set of training sentences and initializing a Tokenizer with a specified num_words (limiting the vocabulary size to the most frequent words) and an oov_token (a special token for words not encountered during training). The tokenizer then learns the vocabulary from the training sentences, converting them into numerical sequences where each number represents a word's unique ID.\n",
    "\n",
    "A key part of the process is pad_sequences, which ensures all text sequences have a uniform length, a common requirement for input to neural networks. Shorter sequences are padded with zeros, and longer ones are truncated.\n",
    "\n",
    "Finally, the script showcases how the trained tokenizer handles new, unseen data, particularly how it uses the <OOV> token to represent words that were not part of its original vocabulary, providing a robust way to manage out-of-vocabulary terms in real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f041b837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Index =  {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "\n",
      "Sequences =  [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
      "\n",
      "Padded Sequences:\n",
      "[[ 0  5  3  2  4]\n",
      " [ 0  5  3  2  7]\n",
      " [ 0  6  3  2  4]\n",
      " [ 9  2  4 10 11]]\n",
      "\n",
      "Test Sequence =  [[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n",
      "\n",
      "Padded Test Sequence: \n",
      "[[0 0 0 0 0 5 1 3 2 4]\n",
      " [0 0 0 0 0 2 4 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Import necessary modules from Keras for text preprocessing.\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define a list of sentences to be processed.\n",
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]\n",
    "\n",
    "# Initialize the Tokenizer.\n",
    "# num_words=100 specifies that the tokenizer will only consider the top 100 most frequent words.\n",
    "# oov_token=\"<OOV>\" defines a special token for \"Out-Of-Vocabulary\" words.\n",
    "# Any word encountered that was not in the original training vocabulary will be replaced by this token's index.\n",
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "\n",
    "# Fit the tokenizer on the provided sentences.\n",
    "# This step analyzes the text, creates a vocabulary of unique words based on frequency,\n",
    "# and assigns a unique integer index to each word. Punctuation is typically removed,\n",
    "# and words are converted to lowercase by default.\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Retrieve the word_index dictionary, which maps each word to its corresponding integer index.\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Convert the sentences into sequences of integers, where each integer represents a word's index.\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Pad the sequences to a uniform length.\n",
    "# maxlen=5 ensures all sequences have a length of 5.\n",
    "# If a sequence is shorter, it will be padded with zeros (by default at the beginning).\n",
    "# If a sequence is longer, it will be truncated (by default from the beginning).\n",
    "padded = pad_sequences(sequences, maxlen=5)\n",
    "\n",
    "# Print the generated word index, original sequences, and padded sequences.\n",
    "print(\"\\nWord Index = \", word_index)\n",
    "print(\"\\nSequences = \", sequences)\n",
    "print(\"\\nPadded Sequences:\")\n",
    "print(padded)\n",
    "\n",
    "# Try with words that the tokenizer wasn't fit to.\n",
    "# These new sentences will demonstrate how the tokenizer handles \"Out-Of-Vocabulary\" words.\n",
    "test_data = [\n",
    "    'i really love my dog',\n",
    "    'my dog loves my manatee'\n",
    "]\n",
    "\n",
    "# Convert the test data sentences into sequences using the already fitted tokenizer.\n",
    "# Words like \"really\" and \"manatee\" were not in the original 'sentences' list,\n",
    "# so they will be replaced by the OOV token's index.\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(\"\\nTest Sequence = \", test_seq)\n",
    "\n",
    "# Pad the test sequences.\n",
    "# maxlen=10 ensures all test sequences have a length of 10.\n",
    "padded = pad_sequences(test_seq, maxlen=10)\n",
    "print(\"\\nPadded Test Sequence: \")\n",
    "print(padded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
